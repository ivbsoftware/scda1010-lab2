---
title: Wine Quality Regression Problem

author: 
  - name          : "Viviane Adohouannon"
    affiliation   : "York University School of Continuing Studies"
    email         : "https://learn.continue.yorku.ca/user/view.php?id=21444"  
  - name          : "Kate Alexander"
    affiliation   : "York University School of Continuing Studies"
    email         : "https://learn.continue.yorku.ca/user/view.php?id=21524"    
  - name          : "Diana Azbel"
    affiliation   : "York University School of Continuing Studies"
    email         : "https://learn.continue.yorku.ca/user/view.php?id=20687"  
  - name          : "Igor Baranov"
    affiliation   : "York University School of Continuing Studies"
    email         : "https://learn.continue.yorku.ca/user/profile.php?id=21219"
abstract: >
  We are using a dataset related to red vinho verde wine samples, from the north of Portugal. The goal is to model wine quality based on physicochemical tests. The classes are ordered and not balanced (e.g. there are munch more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods. The method chosen to solve the problem is Linear Regression.
output:
  rticles::rjournal_article:
    includes:
      in_header: preamble.tex
figsintext        : no
---

## Introduction

Once viewed as a luxury good, nowadays wine is increasingly enjoyed by a
all consumers. Portugal is a top ten wine exporting country with
3.17% of the market share in 2005 \citep{faostat}. Exports of its vinho verde wine (from
the northwest region) have increased by yearly. To support
its growth, the wine industry is investing in new technologies for both wine
making and selling processes. Wine certification and quality assessment are
key elements within this context. Certification prevents the illegal adulteration
of wines (to safeguard human health) and assures quality for the wine market.
Quality evaluation is often part of the certification process and can be used
to improve wine making (by identifying the most influential factors) and to
stratify wines such as premium brands (useful for setting prices).
Wine certification is generally assessed by physicochemical and sensory tests
\citep{teranishi_flavor_1999}. Physicochemical laboratory tests routinely used to characterize wine include
determination of density, alcohol or pH values, while sensory tests rely
mainly on human experts. It should be stressed that taste is the least understood
of the human senses, thus wine classification is a difficult task.
Moreover, the relationships between the physicochemical and sensory analysis
are complex and still not fully understood \citep{legin_evaluation_2003}.

## Background

The two datasets are related to red and white variants of the Portuguese "Vinho Verde" wine. For more details, consult \citep{CorCer09}. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).

These datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are munch more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant.

Due to specific purpose of this lab assignment, we are looking at  Linear Regression problem only using red wine dataset. Full library of the wine datasets and their description are located here: \citep{WineDataset}.

## Objective

The objective of this article is to provide a reliable and feasible recommendation algorithm to predict wine quality based on physicochemical tests.

 * Put all relevant variables in the model
 * Leavethe irrelevant variables out
 * Check linearity
 * Regression assumptions
 
  ** Residuals have a mean of zero
  ** Normality of errors
  ** Residuals are not autocorrelated
  ** Linearityvariables
  ** Need more data than 
  ** No excessive collinearity


# Data understanding
The dataset \citep{WineDataset} has 8 attributes and 12960 instances. The following concept structure:

```
For more information, read [Cortez et al., 2009]. 
Input variables (based on physicochemical tests): 

1 - fixed acidity 
2 - volatile acidity 
3 - citric acid 
4 - residual sugar 
5 - chlorides 
6 - free sulfur dioxide 
7 - total sulfur dioxide 
8 - density 
9 - pH 
10 - sulphates 
11 - alcohol 
Output variable (based on sensory data): 
12 - quality (score between 0 and 10)
```

## Preparation
To perform the analysis, certain R libraries were used. The code below was used to load and initialize the libraries. The first line invoking seed function was applied to enforce the repeatability of the calculation results.

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}
set.seed(42)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(rattle)
library(caret)
```

## Reading red wines dataset

```{r}
library(readr)
wines_red_data <- 
  read.csv(
    "http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv",
    sep=";", 
    header = TRUE, 
    col.names = c("FA","VA","CA","RS","CH","FSD","TSD","DEN","pH","SUL","ALC","QLT"))

str(wines_red_data)
```


```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
library(xtable)
options(xtable.floating = TRUE)
options(xtable.timestamp = "")
options(xtable.comment = FALSE)

dh.rescale <- xtable(head(wines_red_data, n=20), 
  caption = "\\tt Red Wines Quality Dataset - first 20 rows", label = "table:dhead10")
print(dh.rescale, scalebox=1)
```

## Distribution of target value in the dataset
Figure \ref{fig:hist_qlt_rw}:

```{r hist_qlt_rw, fig.pos = 'h', fig.width=6, fig.align="center", fig.cap="Histogram of White Wine QUality"}
h <- hist(wines_red_data$QLT, freq=FALSE, xlab = "Wine Quality (QLT)", main="",
    col = colors()[626])
```

# OLS Modeling
## Default Linear Regression fit
The following code calculates the default OLS model using all the indepedent variables. Results of the calculatiions presented in Table \ref{table:fit1}.
```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
wines_red_data.fit <- lm (QLT ~ ., data=wines_red_data)
# summary(wines_red_data.fit)
xtable(wines_red_data.fit, 
  caption = "\\tt Default OLS model - all variables included", label = "table:fit1")
```
## More detailed summaries

```{r levr1_rw, fig.pos = 'h', fig.width=5, fig.align="center", fig.cap="HIstogram of residuals"}
hist(residuals(wines_red_data.fit), xlab = "Residuals", main = "")
```

## Adjust the fit removing attributes with p > 0.05
```{r}
wines_red_data.fit1 <- lm (QLT ~ VA + CH + FSD + TSD + pH + SUL + ALC,
                           data=wines_red_data)
summary(wines_red_data.fit1)
```
# Stepwise Regression
## Find the best model automatically
```{r message=FALSE, warning=FALSE}
# Stepwise Regression
library(MASS)
fit <- lm(QLT ~ .,data=wines_red_data)
step <- stepAIC(fit, direction="both", trace = FALSE)
step$anova # display results
```

The model identical to the one found the the previous section

## Plot pairwise scatter plots
Pairwise scatter plots \ref{fig:pair_rw}  to inspect the result for relationships between the independent variable and the numerical dependent variables.

```{r pair_rw, fig.height=9, fig.width=6, fig.align="center", fig.cap="Red Wines - relationships between variables"}
attach(wines_red_data)
panel.points<-function(x,y){points(x,y,cex=.1)}
pairs(~QLT + VA + CH + FSD + TSD + pH + SUL + ALC,
      upper.panel=panel.points,lower.panel=panel.points)
```

## Checking correlation matrix
```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE, results='asis'}
cor_rw <- cor(wines_red_data)
table_cor_rw <- xtable(cor_rw, 
  caption = "\\tt Red Wines Quality Dataset Correlation Matrix", label = "table:table_cor_rw")
print(table_cor_rw, scalebox=.9)
```

## Evaluate Nonlinearity component + residual plot 
```{r resplt_rw, fig.width=5.5, fig.height=8, fig.align="center", fig.cap="Red Wines - Component + Resudual Plots"}
library(car)
crPlots(wines_red_data.fit1, layout = c(4,3), main = "")
```
## Assessing Outliers
```{r}
outlierTest(wines_red_data.fit) # Bonferonni p-value for most extreme obs
```
Both p-value and Bonferroni-corrected p-value are smaller that 0.05, so the model is acceptable.

```{r qq_regr_rw, echo=TRUE, fig.align="center", fig.cap="QQ Plot for studentized residuals", fig.pos='h', fig.width=5, fig.height=5, message=FALSE, warning=FALSE}
qqPlot(wines_red_data.fit, main="", ylab="Studentized Residuals" )  #qq plot for studentized resid 
```

A residual plot is a graph that shows the residuals on the vertical axis and the independent variable on the horizontal axis. If the points in a residual plot are randomly dispersed around the horizontal axis, a linear regression model is appropriate for the data; otherwise, a non-linear model is more appropriate. Our model is reasonably dispersed around 0,0 for each of the independent variables <ref>

```{r levr_rw, fig.pos = 'h', fig.width=5.5, fig.height= 9, fig.align="center", fig.cap="Red Wines - Leverage Plots"}
# leverage plots
leveragePlots(wines_red_data.fit, layout = c(4,3), main = "")
```


```{r}
# Test for Autocorrelated Errors
durbinWatsonTest(wines_red_data.fit)
```


```{r}
# Global test of model assumptions
library(gvlma)
gvmodel <- gvlma(wines_red_data.fit1) 
summary(gvmodel)
```
# Addressing Skeweness using log transformation

```{r}
library(car)
summary(wines_red_data.fit2 <- lm (
  bcPower(QLT,1.25)  ~ VA + CH + FSD + TSD + pH + SUL + ALC,
  data=wines_red_data))
gvmodel <- gvlma(wines_red_data.fit2) 
summary(gvmodel)
```


```{r levr2_rw, fig.pos = 'h', fig.width=5, fig.height=4, fig.align="center", fig.cap="HIstogram of residuals after correcting the Skewness"}
hist(residuals(wines_red_data.fit2), xlab = "Residuals", main = "")
```


# Tree-based regression methods
Tree-based methods, while simple and useful for interpretation, are typically not as competitive with the best supervised learning approaches such as polynomial regression. However, tree-based methods such as regression tree and random forests make up for this shortfall. By combining a large number of trees instead of one, the model usually results in dramatic improvements in terms of prediction accuracy. This improvement in accuracy comes at the expense of loss in intepretation.

## Splitting the dataset into train and test
The dataset has been split in such a way that train and test sets would have the same distribution of the 'QLT' attribute. The reason for this stratification strategy is to focus on the priority on the target value. We used 60:34 split ratio. 

```{r message=FALSE, warning=FALSE}
library(caret)
train.rows<- createDataPartition(y= wines_red_data$QLT, p=0.6, list = FALSE)
train.data<- wines_red_data[train.rows,]
prop.table((table(train.data$QLT)))
```

```{r}
test.data<- wines_red_data[-train.rows,]
prop.table((table(test.data$QLT)))
```

## Regression tree fit
In a regression tree, the tree arranges or segments observations into regions of a predictor space. For example, using the "Hitters" data set, which contains various statistics on baseball players, a tree might look something like in Figure \ref{fig:dtree} generated by the code below.

```{r dtree, fig.pos='h', fig.width=5.5, fig.height=4, fig.align="center", fig.cap="Regression Tree Diagram", message=FALSE, warning=FALSE}
library(rpart)
library(rpart.plot)
library(rattle)
library(caret)

reg.tree <- rpart(QLT ~ ., method="anova", data = train.data)
fancyRpartPlot(reg.tree, main="", sub="")
```


```{r dtreeimp}
# reg.tree$variable.importance
```

## Regression Tree model evaluation
This Decision Tree favours the following attributes in order of their importance for the prediction of the target attribute: health, has_nurs, parents. It does not consider the rest of the attributes as important. Let's apply the model to the test set and evaluate accuracy of the predictions. 

```{r }
dtPrediction <- predict(reg.tree, test.data)
cor(dtPrediction,test.data$QLT)
```

```{r plot_rt_rw, fig.pos='h', fig.width=5.5, fig.align="center", message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Regression Tree Prediction"}
plot(jitter(test.data$QLT), dtPrediction, 
     pch=20, col=rgb(0.1, 0.2, 0.8, 0.3), 
     ylab="Prediction", xlab="Test Values", bty="n" )
```

```{r qq_rt_rw, fig.pos='h', fig.width=5.5, fig.align="center", message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Regression Tree Prediction QQ Plot"}
qqPlot(dtPrediction, main="" )
```

## Random Forest model fit

```{r message=FALSE, warning=FALSE}
library(randomForest)
fitRF1 <- randomForest(
  QLT ~ ., method="anova",
  data=train.data, importance=TRUE, ntree=2000)
```

```{r forimp, fig.pos='h', fig.width=5, fig.align="center", message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Importance of the dataset attributes for the prediction of the 'class' attribute"}
varImpPlot(fitRF1, main="")
```
\newpage
## Random Forest model prediction and evaluation

```{r}
PredictionRF1 <- predict(fitRF1, test.data)
cor(PredictionRF1,test.data$QLT)
```

```{r plot_rf_rw, fig.pos='h', fig.width=5.5, fig.align="center", message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Random Forest Prediction"}
plot(jitter(test.data$QLT), PredictionRF1 , 
     pch=20, col=rgb(0.1, 0.2, 0.8, 0.3), 
     ylab="Prediction", xlab="Test Values", bty="n" )
```

```{r qq_rf_rw, fig.pos='h', fig.width=5.5, fig.align="center", message=FALSE, warning=FALSE, paged.print=FALSE, fig.cap="Random Forest Prediction QQ Plot"}
qqPlot(PredictionRF1, main="")
```

\newpage
# Conclusion


\bibliography{RJreferences}

\newpage
# Note from the Authors
This file was generated using [_The R Journal_ style article template](https://github.com/rstudio/rticles), additional information on how to prepare articles for submission is here - [Instructions for Authors](https://journal.r-project.org/share/author-guide.pdf). The article itself is an executable R Markdown file that could be [downloaded from Github](https://github.com/ivbsoftware/scda1010-lab1/tree/master/docs/R_Journal/csda1010-lab1lab1) with all the necessary artifacts.
